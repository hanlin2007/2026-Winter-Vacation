å‚è€ƒäº†è§†é¢‘[Understanding Dropout (C2W1L07)](https://www.youtube.com/watch?v=ARq74QuavAo)

ä¸ºä½•éœ€è¦Dropoutï¼š**è¿‡æ‹Ÿåˆ**

è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªç¥ç»å…ƒä»¥æ¦‚ç‡péšæœºä¸¢å¼ƒï¼š
$$è¾“å‡º = \begin{cases} 0, & \text{ä»¥æ¦‚ç‡p} \\ \frac{åŸå€¼}{1-p}, & \text{ä»¥æ¦‚ç‡1-p} \end{cases}$$

æµ‹è¯•æ—¶ï¼Œæ‰€æœ‰ç¥ç»å…ƒéƒ½å‚ä¸ï¼Œä½†è¾“å‡ºè¦ä¹˜ä»¥(1-p)ä¿æŒæœŸæœ›ä¸€è‡´ã€‚


```python
import torch
import torch.nn as nn

class MLPWithDropout(nn.Module):
    def __init__(self, dropout_rate=0.5):
        super().__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 10)
        self.relu = nn.ReLU()
        
        # ğŸ”‘ Dropoutå±‚
        self.dropout = nn.Dropout(p=dropout_rate)  # pæ˜¯ä¸¢å¼ƒæ¦‚ç‡
        
    def forward(self, x):
        x = self.flatten(x)
        
        # ç¬¬ä¸€å±‚
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)  # éšæœºä¸¢å¼ƒ50%çš„ç¥ç»å…ƒ
        
        # ç¬¬äºŒå±‚
        x = self.fc2(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # è¾“å‡ºå±‚ï¼ˆé€šå¸¸ä¸åŠ dropoutï¼‰
        x = self.fc3(x)
        return x

# è®­ç»ƒæ—¶çš„ä½¿ç”¨
model = MLPWithDropout(dropout_rate=0.5)
model.train()  #è®­ç»ƒæ¨¡å¼ï¼šdropoutç”Ÿæ•ˆ

# æµ‹è¯•æ—¶çš„ä½¿ç”¨
model.eval()   # è¯„ä¼°æ¨¡å¼ï¼šdropoutè‡ªåŠ¨å…³é—­
with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
    test_output = model(test_data)

# **é‡è¦æç¤ºï¼š** 
# PyTorchçš„Dropoutåœ¨è®­ç»ƒæ—¶è‡ªåŠ¨ç¼©æ”¾ï¼Œä¸éœ€è¦æ‰‹åŠ¨å¤„ç†ï¼
```

 **å®ç”¨æŠ€å·§ï¼šä¸åŒå±‚ç”¨ä¸åŒdropoutç‡
```python      
# è¾“å…¥å±‚é™„è¿‘ç”¨å°dropoutï¼Œæ·±å±‚ç”¨å¤§dropout
self.dropout1 = nn.Dropout(0.2)  # ä½å±‚ï¼šä¿ç•™æ›´å¤šä¿¡æ¯
self.dropout2 = nn.Dropout(0.3)  
self.dropout3 = nn.Dropout(0.4)  # é«˜å±‚ï¼šæ›´å¼ºçš„æ­£åˆ™åŒ–
```
